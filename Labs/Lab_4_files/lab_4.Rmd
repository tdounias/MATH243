---
title: "Lab 4--MATH 243"
author: "Theodore Dounias"
date: "October 3, 2017"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
war <- read.csv("http://www.stat.cmu.edu/~cshalizi/uADA/15/hw/06/ch.csv", row.names = 1)
library(tidyverse)
library(glmnet)
library(MASS)
attach(war)
```

**1**
     
```{r}
war <- war %>%
  mutate(exports_quad = exports^2)

cw_logit <- glm(start ~ exports + schooling + growth + peace + concentration + lnpop + fractionalization + dominance + exports_quad, family = binomial, data = war)

summary(cw_logit)
```
    
All predictors apart from dominance are statistically significant at the 5% level.
   
**2**
   
```{r}
#1
cw_logit <- glm(start ~ exports + schooling + growth + peace + concentration + lnpop + fractionalization + exports_quad, family = binomial, data = war)

ind_1975 <- war %>%
  filter(country == "India", year == "1975")

predict(cw_logit, type = "response", ind_1975)

ind_1975[2] <- ind_1975[2] + 30

predict(cw_logit, type = "response", ind_1975)

ind_1975 <- war %>%
  filter(country == "India", year == "1975") 

ind_1975[1] <- ind_1975[1] + 0.1

predict(cw_logit, type = "response", ind_1975)

#2

nig_1965 <- war %>%
  filter(country == "Nigeria", year == "1965")

predict(cw_logit, type = "response", nig_1965)

nig_1965[2] <- nig_1965[2] + 30

predict(cw_logit, type = "response", nig_1965)

nig_1965 <- war %>%
  filter(country == "Nigeria", year == "1965") 

nig_1965[1] <- nig_1965[1] + 0.1

predict(cw_logit, type = "response", nig_1965)
```
    
The difference in probabilities is not the same because we are dealing with log odds, not with a linear association. Therefore a change in a variable will not have the same effect ranging between two different observations. The function for getting a probability here is dependant on both the relative difference--here what we add to the observed values--and the prior state of the values of the observations.
    
**3**
    
```{r}
#1
my_log_pred <- ifelse(cw_logit$fit < 0.5, "No", "Yes")
Civ_war <- mutate(na.omit(war), civ_war_Yes = ifelse(start == 1, "Yes", "No"))

conf_log <- table(my_log_pred, Civ_war$civ_war_Yes)
conf_log

#2
(1/nrow(Civ_war)) * (conf_log[2, 1] + conf_log[1, 2])


#3
count <- war %>%
  filter(!is.na(start)) %>%
  summarize(n_start = sum(start))

(1/sum(!is.na(start)))*count[1, 1]

(1/nrow(Civ_war))*sum(Civ_war$civ_war_Yes == "Yes")
```
     
**4**
```{r}
#1
cw_lda <- lda(start ~ exports + schooling + growth + peace + concentration + lnpop + fractionalization + dominance + exports_quad, data = war)

lda_pred <- predict(cw_lda)

conf_lda <- table(lda_pred$class, Civ_war$civ_war_Yes)

(1/nrow(Civ_war)) * (conf_lda[2, 1] + conf_lda[1, 2])

#2
cw_qda <- lda(start ~ exports + schooling + growth + peace + concentration + lnpop + fractionalization + dominance + exports_quad, data = war)

qda_pred <- predict(cw_qda)

conf_qda <- table(qda_pred$class, Civ_war$civ_war_Yes)

(1/nrow(Civ_war)) * (conf_qda[2, 1] + conf_qda[1, 2])
```
   
LDA and QDA perform similarly well, and Logistic Regression performs slightly less so. LDA and QDA performing in a similar fashion indicates that the data we have can be model so that our predictors share a covariance matrix, without incurring too much bias. There is no need to use a less parsimonious model such as QDA if LDA performs just as well (although bear in mind this is training, not test data). Logit performs worse than both of these because it entails calculating more parameters for each redictor, and is less stable than LDA when the data is approximatelly normaly distributed, which it seems to be here.
